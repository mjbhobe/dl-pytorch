{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pytorch version 2.0.1+cu117. GPU is not available :(\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import random\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "\n",
    "print(\n",
    "    f\"Using Pytorch version {torch.__version__}. \"\n",
    "    + f'GPU {\"is available :)\" if torch.cuda.is_available() else \"is not available :(\"}'\n",
    ")\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torchmetrics\n",
    "import torchsummary\n",
    "\n",
    "# My helper functions for training/evaluating etc.\n",
    "import torch_training_toolkit as t3\n",
    "\n",
    "SEED = t3.seed_all()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence    Wow... Loved this place.\n",
      "label                              1\n",
      "source                          yelp\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "DATASET_BASE_PATH = pathlib.Path(os.getcwd()) / \"data\" / \"sentiment_labelled_sentences\"\n",
    "assert os.path.exists(DATASET_BASE_PATH), f\"FATAL: {DATASET_BASE_PATH} - path does not exist!\"\n",
    "\n",
    "dataset_paths = {\n",
    "    \"yelp\": DATASET_BASE_PATH / \"yelp_labelled.txt\",\n",
    "    \"amazon\": DATASET_BASE_PATH / \"amazon_cells_labelled.txt\",\n",
    "    \"imdb\": DATASET_BASE_PATH / \"imdb_labelled.txt\",\n",
    "}\n",
    "\n",
    "df_list = []\n",
    "for source, filepath in dataset_paths.items():\n",
    "    df = pd.read_csv(str(filepath), names=[\"sentence\", \"label\"], sep=\"\\t\")\n",
    "    df[\"source\"] = source\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how sklearn's CountVectorizer helps us build a vocab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\"John likes ice cream!\", \"John hates chocolate.\"]\n",
    "vectorizer = CountVectorizer(min_df=0.0, lowercase=False)\n",
    "vectorizer.fit(sentences)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is also called bag-of-words technique\n",
    "vectorizer.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us perform the same action on our entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Wow... Loved this place.', 'Crust is not good.', 'Not tasty and the texture was just nasty.', 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.', 'The selection on the menu was great and so were the prices.'], dtype=object),\n",
       " array([1, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df[\"sentence\"].values\n",
    "labels = df[\"label\"].values\n",
    "sentences[:5], labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train & test datasets for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (2061,) - y_train.shape: (2061,) - X_test.shape: (687,) - y_test.shape: (687,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sentences,\n",
    "    labels,\n",
    "    test_size=0.25,\n",
    "    random_state=SEED,\n",
    ")\n",
    "print(\n",
    "    f\"X_train.shape: {X_train.shape} - y_train.shape: {y_train.shape} - \"\n",
    "    f\"X_test.shape: {X_test.shape} - y_test.shape: {y_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x4475 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 16 stored elements in Compressed Sparse Row format>,\n",
       " <1x4475 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 11 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we will vectorize the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base `LogisticRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression (base model) acc: 0.806\n"
     ]
    }
   ],
   "source": [
    "# base classification model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(f\"LogisticRegression (base model) acc: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a deep-learning model with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = pathlib.Path(os.getcwd()) / \"model_states\" / \"pyt_text_classfication.pt\"\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            t3.Linear(input_dim, 10),\n",
    "            nn.ReLU(),\n",
    "            t3.Linear(10, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "metrics_map = {\n",
    "    \"acc\": torchmetrics.classification.BinaryAccuracy(),\n",
    "}\n",
    "trainer = t3.Trainer(\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    "    metrics_map=metrics_map,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got csr_matrix)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m Net()\n\u001b[1;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m hist \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      5\u001b[0m     model,\n\u001b[1;32m      6\u001b[0m     optimizer,\n\u001b[1;32m      7\u001b[0m     train_dataset\u001b[39m=\u001b[39;49m(X_train, y_train),\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m hist\u001b[39m.\u001b[39mplot_metrics(\n\u001b[1;32m     10\u001b[0m     title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel Performance\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     fig_size\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m),\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m t3\u001b[39m.\u001b[39msave_model(model, MODEL_SAVE_PATH)\n",
      "File \u001b[0;32m~/code/git-projects/dl-pytorch/torch_training_toolkit/training.py:646\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, optimizer, train_dataset, validation_dataset, validation_split, l1_reg, lr_scheduler, early_stopping, verbose)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39massert\u001b[39;00m train_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mFATAL ERROR: Trainer.fit() -> \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot be None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m \u001b[39m# if lr_scheduler is not None:\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39m#     # NOTE:  ReduceLROnPlateau is NOT derived from _LRScheduler, but from object, which\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39m#     # is odd as all other schedulers derive from _LRScheduler\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39m#         \"lr_scheduler: incorrect type. Expecting class derived from \" \\\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m#         \"torch.optim._LRScheduler or ReduceLROnPlateau\"\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m history \u001b[39m=\u001b[39m cross_train_module(\n\u001b[1;32m    647\u001b[0m     model,\n\u001b[1;32m    648\u001b[0m     train_dataset,\n\u001b[1;32m    649\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn,\n\u001b[1;32m    650\u001b[0m     optimizer,\n\u001b[1;32m    651\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    652\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    653\u001b[0m     validation_dataset\u001b[39m=\u001b[39;49mvalidation_dataset,\n\u001b[1;32m    654\u001b[0m     metrics_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetrics_map,\n\u001b[1;32m    655\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs,\n\u001b[1;32m    656\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    657\u001b[0m     l1_reg\u001b[39m=\u001b[39;49ml1_reg,\n\u001b[1;32m    658\u001b[0m     reporting_interval\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreporting_interval,\n\u001b[1;32m    659\u001b[0m     lr_scheduler\u001b[39m=\u001b[39;49mlr_scheduler,\n\u001b[1;32m    660\u001b[0m     early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m    661\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshuffle,\n\u001b[1;32m    662\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[1;32m    663\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    664\u001b[0m )\n\u001b[1;32m    665\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/code/git-projects/dl-pytorch/torch_training_toolkit/training.py:71\u001b[0m, in \u001b[0;36mcross_train_module\u001b[0;34m(model, dataset, loss_fxn, optimizer, device, validation_split, validation_dataset, metrics_map, epochs, batch_size, l1_reg, reporting_interval, lr_scheduler, early_stopping, shuffle, num_workers, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m train_dataset, val_dataset \u001b[39m=\u001b[39m dataset, validation_dataset\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(train_dataset, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[39m# train dataset was a tuple of np.ndarrays - convert to Dataset\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     torch_X_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(train_dataset[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)\n\u001b[1;32m     72\u001b[0m     torch_y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(train_dataset[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mtype(\n\u001b[1;32m     73\u001b[0m         torch\u001b[39m.\u001b[39mLongTensor \u001b[39mif\u001b[39;00m train_dataset[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [np\u001b[39m.\u001b[39mint, np\u001b[39m.\u001b[39mlong] \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mFloatTensor\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m     train_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(torch_X_train, torch_y_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got csr_matrix)"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "hist = trainer.fit(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataset=(X_train, y_train),\n",
    ")\n",
    "hist.plot_metrics(\n",
    "    title=\"Model Performance\",\n",
    "    fig_size=(16, 8),\n",
    ")\n",
    "t3.save_model(model, MODEL_SAVE_PATH)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
